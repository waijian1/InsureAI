{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe1e69d-a56a-433c-abf5-92925a6c9b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waijianlim/miniconda3/envs/DeepSeek/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'home_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define the path to save the fine-tuned model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mhome_dir\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine_tuned_model\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Points to InsureAI/models/fine_tuned_model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m load_model \u001b[38;5;241m=\u001b[39m model_dir\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the fine-tuned model and tokenizer\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'home_dir' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Define the path to save the fine-tuned model\n",
    "home_dir = os.path.expanduser('~/InsureAI')\n",
    "model_dir = os.path.join(home_dir, \"models\", \"fine_tuned_model\") # Points to InsureAI/models/fine_tuned_model\n",
    "load_model = model_dir\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(load_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb14234a-c435-49c5-a823-769b61e8d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_response(prompt, max_length=100):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d640b6-14d7-492a-97e1-928280963854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Prompt:\n",
      "What is equal to 1+1?\n",
      "### Response:\n",
      "The sum of two ones is two, so 1+1 equals two.\n",
      "\n",
      "Alright, I'm trying to figure out how to respond to this instruction. The user provided a prompt asking \"What is equal to 1+1?\" and the response they wrote is: \"The sum of two ones is two, so 1\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is equal to 1+1?\"\n",
    "response = generate_response(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d2e612-a661-4f15-b551-56b491e9ec64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Prompt:\n",
      "Describe the insurance product Death A offered by InsureAI\n",
      "### Response:\n",
      "[Your Answer]\n",
      "\n",
      "[Options]\n",
      "A) The insurance product Death A is a traditional insurance policy with a fixed term and no coverage for accidental damage.\n",
      "B) The insurance product Death A is a traditional insurance policy with a fixed term and no coverage for accidental damage.\n",
      "C) The insurance product Death A is a traditional\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Describe the insurance product Death A offered by InsureAI\"\n",
    "response = generate_response(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8da94b-abfa-449f-bc24-751b7c01639a",
   "metadata": {},
   "source": [
    "Inference from original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19783fdb-f2cc-42ee-a72e-af6972643f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To query two table same column in SQL, you can use the SELECT statement with the column name as the common keyword. For example, to query two tables with the same column name 'id', you can use the following SQL query:\n",
      "\n",
      "SELECT id FROM table1 id, id FROM table2 id\n",
      "</think>\n",
      "\n",
      "To query two tables with the same column name in SQL, you can use the `SELECT` statement with the column name as a common keyword. Here's an example:\n",
      "\n",
      "```sql\n",
      "SELECT id FROM table1 id, id FROM table2 id\n",
      "```\n",
      "\n",
      "This query will return the `id` values from both `table1` and `table2`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)  # Move model to GPU\n",
    "max_new_tokens=1000\n",
    "\n",
    "# Custom stopping criteria for streaming\n",
    "class StopOnMaxTokens(StoppingCriteria):\n",
    "    def __init__(self, max_new_tokens):\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.current_tokens = 0\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        self.current_tokens += 1\n",
    "        return self.current_tokens >= self.max_new_tokens\n",
    "\n",
    "def generate_response_streaming(prompt, model, tokenizer, max_new_tokens=max_new_tokens):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Streaming-based generation\n",
    "    stop_criteria = StoppingCriteriaList([StopOnMaxTokens(max_new_tokens)])\n",
    "    \n",
    "    generated_ids = input_ids  # Start with prompt\n",
    "    # print(\"\\nAssistant:\", end=\" \", flush=True)\n",
    "\n",
    "    skip_tokens = {\"<think>\", \".</think>\", \".\", \"</think>\"}  # Define unwanted tokens\n",
    "    skip_mode = True  # Start in skip mode\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=generated_ids)  # Get logits\n",
    "            next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1, keepdim=True)  # Get top token\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:  # Stop on EOS token\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat((generated_ids, next_token_id), dim=1)  # Append token\n",
    "\n",
    "        # Decode new token properly\n",
    "        new_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "\n",
    "        if skip_mode:\n",
    "            if new_token.strip() in skip_tokens:  # Skip unwanted tokens\n",
    "                continue\n",
    "            else:\n",
    "                skip_mode = False  # Stop skipping once we see the real answer\n",
    "                new_token = new_token.lstrip() # Remove leading space from the first word\n",
    "\n",
    "        # Print token **without breaking spaces or newlines**\n",
    "        print(new_token, end=\"\", flush=True)\n",
    "\n",
    "        torch.cuda.synchronize()  # Ensure GPU processes before printing\n",
    "\n",
    "    # print(\"\\n\")  # Newline after response\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"How to query two table same column in SQL?\"\n",
    "generate_response_streaming(f'Answer directly without providing your thinking or reasoning. Keep your answer in {max_new_tokens - 100} max tokens.\\n\\n{prompt}', model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
