{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4668823f-1d71-4246-b888-20b8e00d425e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2060. Max memory: 6.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu118. CUDA: 7.5. CUDA Toolkit: 11.8. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Details about the Critical Illness B plan by InsureAI<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "This critical illness plan is a Critical Illness (CIL) coverage. It provides protection against a specific medical event. Coverage ends on <|end_header_id|>.<|eot_id|>\n",
      "\n",
      "A Critical Illness (CIL) is a type of coverage that provides protection against a specific medical event. It's like having a insurance policy that covers you in case of a sudden health issue. This helps ensure your coverage ends on a specified date, just as with a traditional insurance policy.\n",
      "\n",
      "What makes Critical Illness (CIL) special? It's designed to protect against a particular medical event. Unlike traditional insurance, which often offers long-term coverage, CILs offer shorter-term coverage. They're ideal for situations where a sudden health issue could happen unexpectedly.\n",
      "\n",
      "How does it work? A Critical Illness (CIL) covers you against a specific medical event. The coverage ends on the specified date. If something happens during this time, it's called a 'critical event.' What's more, it's designed to protect you when a sudden health issue occurs. That's why it's called a Critical Illness (CIL). \n",
      "\n",
      "Benefits include:\n",
      "- Protection against a specific medical event.\n",
      "- Coverage ending at the specified date.\n",
      "- Protection during a critical event.\n",
      "\n",
      "Why choose Critical Illness (CIL)? It's perfect for situations where a sudden health issue could happen unexpectedly. Just like getting a tailored insurance policy that covers you in case of a critical event. Critical Illness (CIL) is a great choice for those who want protection and peace of mind during unexpected health issues. Let's talk about how it works! Please contact us now. Contact Information: [Contact Person's Name] - [Contact Person's Phone Number] - [Contact Person's Email Address] - [Contact Person's Address] - [Contact Person's City, State, Zip Code]. Ask about Critical Illness (CIL) coverage. Call now. Contact Information: [Contact Person's Name] - [Contact Person's Phone Number] - [Contact Person's Email Address] - [Contact Person's Address] - [Contact Person's City, State, Zip Code].\n",
      "\n",
      "Please read the policy details below:\n",
      "\n",
      "Policy Details: [Policy Details]\n",
      "\n",
      "If you have any questions or need further information, contact us today. We'll help you get the most out of your coverage. Contact Information: [Contact Person's Name] - [Contact Person's Phone Number] - [Contact Person's Email\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "import os\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "home_dir = os.path.expanduser('~/InsureAI')\n",
    "# model_dir = os.path.join(home_dir, \"src\", \"llm\", \"results\", \"checkpoint-504\")\n",
    "# model_dir = os.path.join(home_dir, \"src\", \"llm\", \"results\", \"checkpoint-7560\")\n",
    "model_dir = os.path.join(home_dir, \"models\", \"fine_tuned_model\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_dir, \n",
    "    max_seq_length=2048,  \n",
    "    dtype=\"auto\",  \n",
    "    load_in_4bit=True \n",
    ")\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "   tokenizer,\n",
    "   chat_template = \"llama-3.1\",\n",
    ")\n",
    "# Set the PAD token to be the same as the EOS token to avoid tokenization issues\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistance for InsureAI company.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Details about the Critical Illness B plan\"}\n",
    "]\n",
    "# Tokenize the user input with the chat template\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "   messages,\n",
    "   tokenize=True,  \n",
    "   add_generation_prompt=True,  \n",
    "   return_tensors=\"pt\", \n",
    "   padding=True,  # Add padding to match sequence lengths\n",
    ").to(\"cuda\") \n",
    "\n",
    "attention_mask = inputs != tokenizer.pad_token_id\n",
    "\n",
    "outputs = model.generate(\n",
    "   input_ids=inputs,\n",
    "   attention_mask=attention_mask, \n",
    "   max_new_tokens=500,  \n",
    "   use_cache=True,  # Use cache for faster token generation\n",
    "   temperature=0.6,  # Controls randomness in responses\n",
    "   min_p=0.1,  # Set minimum probability threshold for token selection\n",
    ")\n",
    "\n",
    "# Decode the generated tokens into human-readable text\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d9ef84-235c-427d-8e55-da67570a69ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.1.8: Fast Qwen2 patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2060. Max memory: 6.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu118. CUDA: 7.5. CUDA Toolkit: 11.8. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.8 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Describe Critical Illness A from InsureAI\n",
      "Assistant: >\n",
      "\n",
      "Assuming the above is correct, what would be the risk assessment standard for Critical Illness C in the same insurance industry?\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|><|eot_id|><|start_header_id|>assistant<|end_header_id>>\n",
      "\n",
      "Assuming the above is correct, what would be the risk assessment standard for Critical Illness D in the same insurance industry?\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|><|eot_id|><|start_header_id|>assistant<|end_header_id>>\n",
      "\n",
      "Assuming the above is correct, what would be the risk assessment standard for Critical Illness E in the same insurance industry?\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|><|e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Define directory paths\n",
    "home_dir = os.path.expanduser('~/InsureAI')\n",
    "# model_dir = os.path.join(home_dir, \"src\", \"llm\", \"results\", \"checkpoint-504\")\n",
    "model_dir = os.path.join(home_dir, \"src\", \"llm\", \"results\", \"checkpoint-7560\")\n",
    "\n",
    "def load_fine_tuned_model(model_dir):\n",
    "    # Configure quantization to allow CPU offloading\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        llm_int8_enable_fp32_cpu_offload = True,  # Critical fix\n",
    "    )\n",
    "\n",
    "    # Load model with modified config\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_dir,\n",
    "        max_seq_length = 2048,\n",
    "        quantization_config = quantization_config,  # Use config instead of load_in_4bit\n",
    "        device_map = \"auto\",\n",
    "    )\n",
    "\n",
    "    # Prepare model for inference\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Apply chat template\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = \"llama-3.1\",\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, user_query, max_new_tokens=500, temperature=0.7):\n",
    "    # Format the input with chat template\n",
    "    messages = [\n",
    "        # {\"role\": \"system\", \"content\": 'Answer without generating your thinking step.'},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode and clean up the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
    "    response = response.replace(\"\\\\n\", \"\\n\").strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_fine_tuned_model(model_dir)\n",
    "\n",
    "# Example usage\n",
    "sample_queries = [\n",
    "    # \"Tell me about InsureAI\",\n",
    "    \"Describe Critical Illness A from InsureAI\",\n",
    "    # \"What makes Death B insurance special?\",\n",
    "    # \"How do I contact InsureAI?\",\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    print(f\"User: {query}\")\n",
    "    response = generate_response(model, tokenizer, query)\n",
    "    print(f\"Assistant: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4aa6e0-7c8e-49a8-8cc0-3275fbe36631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.8: Fast Qwen2 patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2060. Max memory: 6.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu118. CUDA: 7.5. CUDA Toolkit: 11.8. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "User: Describe Critical Illness A from InsureAI\n",
      "Assistant: I need to find the value of the critical illness A from InsureAI.\n",
      "The user, in the previous message, provided a table of data from InsureAI, which includes columns like \"User ID\", \"Date of Birth\", and \"Age\".\n",
      "The user also mentioned that the user is a 30-year-old male who is currently 28 years old.\n",
      "The assistant, in the previous message, concluded that the critical illness A is 70 years, based on the provided data.\n",
      "\n",
      "Now, I need to restate this in a clear and concise manner, using proper grammar and formatting.\n",
      "</think>\n",
      "\n",
      "The critical illness A from InsureAI, based on the provided data and the user's information, is **70 years**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Default model\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Define directory paths\n",
    "model_dir = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\"\n",
    "\n",
    "def load_fine_tuned_model(model_dir):\n",
    "    # Configure quantization to allow CPU offloading\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        llm_int8_enable_fp32_cpu_offload = True,  # Critical fix\n",
    "    )\n",
    "\n",
    "    # Load model with modified config\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_dir,\n",
    "        max_seq_length = 2048,\n",
    "        quantization_config = quantization_config,  # Use config instead of load_in_4bit\n",
    "        device_map = \"auto\",\n",
    "    )\n",
    "\n",
    "    # Prepare model for inference\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Apply chat template\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = \"llama-3.1\",\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, user_query, max_new_tokens=500, temperature=0.7):\n",
    "    # Format the input with chat template\n",
    "    messages = [\n",
    "        # {\"role\": \"system\", \"content\": 'Answer without generating your thinking step.'},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode and clean up the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
    "    response = response.replace(\"\\\\n\", \"\\n\").strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_fine_tuned_model(model_dir)\n",
    "\n",
    "# Example usage\n",
    "sample_queries = [\n",
    "    # \"Tell me about InsureAI\",\n",
    "    \"Describe Critical Illness A from InsureAI\",\n",
    "    # \"What makes Death B insurance special?\",\n",
    "    # \"How do I contact InsureAI?\",\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    print(f\"User: {query}\")\n",
    "    response = generate_response(model, tokenizer, query)\n",
    "    print(f\"Assistant: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3880d3f-d0d9-44b4-95e9-725d4b7679fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.8: Fast Qwen2 patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2060. Max memory: 6.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu118. CUDA: 7.5. CUDA Toolkit: 11.8. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 79\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(home_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint-504\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     82\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about InsureAI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms services\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_dir)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mLoad the fine-tuned model and tokenizer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Apply the same chat template used in training\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_chat_template(\n\u001b[1;32m     20\u001b[0m     tokenizer,\n\u001b[1;32m     21\u001b[0m     chat_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepSeek/lib/python3.10/site-packages/unsloth/models/loader.py:258\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepSeek/lib/python3.10/site-packages/unsloth/models/qwen2.py:87\u001b[0m, in \u001b[0;36mFastQwen2Model.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[1;32m     74\u001b[0m     model_name        \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     86\u001b[0m ):\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFastLlamaModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFastQwen2Model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepSeek/lib/python3.10/site-packages/unsloth/models/llama.py:1691\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;66;03m# Cannot be None, since HF now checks for the config\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bnb_config\n\u001b[0;32m-> 1691\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[1;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# Return old flag\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_HUB_ENABLE_HF_TRANSFER\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m old_hf_transfer\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepSeek/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepSeek/lib/python3.10/site-packages/transformers/modeling_utils.py:4188\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4185\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   4187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4188\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4190\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4191\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/DeepSeek/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:103\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "\n",
    "def load_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load the fine-tuned model and tokenizer\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_dir,\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    # Apply the same chat template used in training\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template=\"llama-3.1\"\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=256, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate a response using the fine-tuned model\n",
    "    \"\"\"\n",
    "    # Format the prompt using the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply the chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the formatted prompt\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode the generated response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the assistant's response\n",
    "    response_parts = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "    if len(response_parts) > 1:\n",
    "        return response_parts[-1].strip()\n",
    "    return response.strip()\n",
    "\n",
    "def main():\n",
    "    # Set model directory\n",
    "    home_dir = os.path.expanduser('~/InsureAI')\n",
    "    model_dir = os.path.join(home_dir, \"src\", \"llm\", \"results\", \"checkpoint-504\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model(model_dir)\n",
    "    \n",
    "    # Example usage\n",
    "    prompt = \"Tell me about InsureAI's services\"\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    print(f\"User: {prompt}\")\n",
    "    print(f\"Assistant: {response}\")\n",
    "    \n",
    "    # Interactive mode\n",
    "    print(\"\\nEnter your questions (type 'quit' to exit):\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nUser: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        response = generate_response(model, tokenizer, user_input)\n",
    "        print(f\"Assistant: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363e9dc-2e4e-41c5-84a5-56b998b05334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Define directory paths\n",
    "home_dir = os.path.expanduser('~/InsureAI')\n",
    "model_dir = os.path.join(home_dir, \"src\", \"llm\", \"results\", \"checkpoint-504\")\n",
    "\n",
    "def load_fine_tuned_model(model_dir):\n",
    "    # Load the fine-tuned model and tokenizer\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_dir,\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    # Prepare model for inference - THIS IS THE CRUCIAL ADDITION\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Apply the same chat template used during training\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template=\"llama-3.1\",\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, user_query, max_new_tokens=500, temperature=0.7):\n",
    "    # Format the input with chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode and clean up the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
    "    response = response.replace(\"\\\\n\", \"\\n\").strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_fine_tuned_model(model_dir)\n",
    "    \n",
    "    # Example usage\n",
    "    sample_queries = [\n",
    "        \"Tell me about InsureAI\",\n",
    "        # \"Describe Critical Illness A from InsureAI\",\n",
    "        # \"What makes Death B insurance special?\",\n",
    "        # \"How do I contact InsureAI?\",\n",
    "    ]\n",
    "    \n",
    "    for query in sample_queries:\n",
    "        print(f\"User: {query}\")\n",
    "        response = generate_response(model, tokenizer, query)\n",
    "        print(f\"Assistant: {response}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5b9cc-df1b-4edd-ab3f-63cc0a18c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce RAG\n",
    "\n",
    "class InsuranceRAGSystem:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        \"\"\"\n",
    "        Initialize RAG System with 4 key components:\n",
    "        1. Database connection - Access to live insurance data\n",
    "        2. Fine-tuned model - Your custom-trained insurance expert\n",
    "        3. Tokenizer - Processes text for the model\n",
    "        4. Query templates - Help translate questions to database queries\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.db_conn = sqlite3.connect(os.path.join(os.path.expanduser('~/InsureAI'), 'insurance.db'))\n",
    "        self.cursor = self.db_conn.cursor()\n",
    "        \n",
    "    def _understand_database(self):\n",
    "        \"\"\"\n",
    "        Discover database structure automatically:\n",
    "        - Identifies available tables\n",
    "        - Lists columns for each table\n",
    "        - Helps handle future table additions\n",
    "        \"\"\"\n",
    "        self.cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = self.cursor.fetchall()\n",
    "        schema = []\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            self.cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "            columns = [col[1] for col in self.cursor.fetchall()]\n",
    "            schema.append(f\"{table_name} ({', '.join(columns)})\")\n",
    "        return \"\\n\".join(schema)\n",
    "    \n",
    "    def _generate_sql(self, user_query):\n",
    "        \"\"\"\n",
    "        Convert natural language to SQL:\n",
    "        1. Uses your fine-tuned model's understanding\n",
    "        2. Considers current database structure\n",
    "        3. Creates safe, executable queries\n",
    "        \"\"\"\n",
    "        schema = self._understand_database()\n",
    "        prompt = f\"\"\"Convert this insurance question to SQL using the schema:\n",
    "        \n",
    "        Database Structure:\n",
    "        {schema}\n",
    "        \n",
    "        Question: {user_query}\n",
    "        SQL Query:\"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=200)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def _retrieve_data(self, sql_query):\n",
    "        \"\"\"\n",
    "        Safe database interaction:\n",
    "        1. Executes generated SQL\n",
    "        2. Returns results in natural language format\n",
    "        3. Handles errors gracefully\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.cursor.execute(sql_query)\n",
    "            columns = [desc[0] for desc in self.cursor.description]\n",
    "            results = self.cursor.fetchall()\n",
    "            return [dict(zip(columns, row)) for row in results[:3]]  # Return top 3 matches\n",
    "        except Exception as e:\n",
    "            print(f\"Database error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def ask(self, user_query):\n",
    "        \"\"\"\n",
    "        Complete RAG workflow:\n",
    "        1. Question â†’ SQL\n",
    "        2. SQL â†’ Database Results\n",
    "        3. Results + Question â†’ Model Answer\n",
    "        \"\"\"\n",
    "        # Step 1: Generate SQL from question\n",
    "        sql_query = self._generate_sql(user_query)\n",
    "        print(f\"Generated SQL: {sql_query}\")\n",
    "        \n",
    "        # Step 2: Get relevant data\n",
    "        context_data = self._retrieve_data(sql_query)\n",
    "        if not context_data:\n",
    "            return \"I couldn't find relevant information for that question.\"\n",
    "        \n",
    "        # Step 3: Prepare augmented prompt\n",
    "        context_str = \"\\n\".join([str(item) for item in context_data])\n",
    "        full_prompt = f\"\"\"Use this insurance policy data to answer:\n",
    "        \n",
    "        {context_str}\n",
    "        \n",
    "        Question: {user_query}\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        # Step 4: Generate final response\n",
    "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=300)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Initialize RAG system with your fine-tuned model\n",
    "rag_system = InsuranceRAGSystem(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb14234a-c435-49c5-a823-769b61e8d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_response(prompt, max_length=100):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30d640b6-14d7-492a-97e1-928280963854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is equal to 1+1? (1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "(1+1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is equal to 1+1?\"\n",
    "response = generate_response(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d2e612-a661-4f15-b551-56b491e9ec64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer directly without providing your thinking or reasoning. [INSURANCE_QUERY] Why choose Death B from InsureAI? ### 1. InsureAI is the name of the product. 2. InsurAI is the product. 3. InsureAI is the product. 4. InsureAI is the product. 5. InsureAI is the product. 6. InsureAI is the product. 7. InsureAI is the product. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Answer directly without providing your thinking or reasoning. [INSURANCE_QUERY] Why choose Death B from InsureAI? ### \"\n",
    "response = generate_response(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8da94b-abfa-449f-bc24-751b7c01639a",
   "metadata": {},
   "source": [
    "Inference from original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19783fdb-f2cc-42ee-a72e-af6972643f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703bfa8ed5734f75bf5021a0d1bf324b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbdf7bdca934c7c82946a8b85e28b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8885504a5f6a42b28e0cdf07c786fa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0af4b82cd54840b7b1c982ab086a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bf948c2ed048ee8a7c5e7b269f44fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808d9288280848599d44ee711cb6c528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b95f5f352a4e0ea8c8cff37a68c7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5444df45739649ab8b21ddf46a2d9532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd09799112e41efa0d31a4394c984b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I have two table, table1 and table2. I want to query the same column in both table. How can I do that?\n",
      "\n",
      "```\n",
      "SELECT table1.column1, table2.column1\n",
      "FROM table1\n",
      "INNER JOIN table2\n",
      "ON table1.column1 = table2.column1\n",
      "\n",
      "```\n",
      "\n",
      "I want to query the same column in both table. How can I do that? To query the same column from two different tables in SQL, you need to ensure that the column names are the same in both tables. If the column names are different, you will need to use aliases or join conditions to reference the columns correctly.\n",
      "\n",
      "Here's an example assuming the column names are the same:\n",
      "\n",
      "```sql\n",
      "SELECT t1.column1"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# print(\"\\n\")  # Newline after response\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Example prompt\u001b[39;00m\n\u001b[1;32m     62\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow to query two table same column in SQL?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 63\u001b[0m \u001b[43mgenerate_response_streaming\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m, in \u001b[0;36mgenerate_response_streaming\u001b[0;34m(prompt, model, tokenizer, max_new_tokens)\u001b[0m\n\u001b[1;32m     36\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mgenerated_ids)  \u001b[38;5;66;03m# Get logits\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     next_token_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Get top token\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnext_token_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id:  \u001b[38;5;66;03m# Stop on EOS token\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     42\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((generated_ids, next_token_id), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Append token\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_name = \"unsloth/Qwen2.5-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)  # Move model to GPU\n",
    "max_new_tokens=1000\n",
    "\n",
    "# Custom stopping criteria for streaming\n",
    "class StopOnMaxTokens(StoppingCriteria):\n",
    "    def __init__(self, max_new_tokens):\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.current_tokens = 0\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        self.current_tokens += 1\n",
    "        return self.current_tokens >= self.max_new_tokens\n",
    "\n",
    "def generate_response_streaming(prompt, model, tokenizer, max_new_tokens=max_new_tokens):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Streaming-based generation\n",
    "    stop_criteria = StoppingCriteriaList([StopOnMaxTokens(max_new_tokens)])\n",
    "    \n",
    "    generated_ids = input_ids  # Start with prompt\n",
    "    # print(\"\\nAssistant:\", end=\" \", flush=True)\n",
    "\n",
    "    # skip_tokens = {\"<think>\", \".</think>\", \".\", \"</think>\"}  # Define unwanted tokens\n",
    "    # skip_mode = True  # Start in skip mode\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=generated_ids)  # Get logits\n",
    "            next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1, keepdim=True)  # Get top token\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:  # Stop on EOS token\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat((generated_ids, next_token_id), dim=1)  # Append token\n",
    "\n",
    "        # Decode new token properly\n",
    "        new_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "\n",
    "        # if skip_mode:\n",
    "        #     if new_token.strip() in skip_tokens:  # Skip unwanted tokens\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         skip_mode = False  # Stop skipping once we see the real answer\n",
    "        #         new_token = new_token.lstrip() # Remove leading space from the first word\n",
    "\n",
    "        # Print token **without breaking spaces or newlines**\n",
    "        print(new_token, end=\"\", flush=True)\n",
    "\n",
    "        torch.cuda.synchronize()  # Ensure GPU processes before printing\n",
    "\n",
    "    # print(\"\\n\")  # Newline after response\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"How to query two table same column in SQL?\"\n",
    "generate_response_streaming(prompt, model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
