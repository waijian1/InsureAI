{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe1e69d-a56a-433c-abf5-92925a6c9b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Define the path to save the fine-tuned model\n",
    "home_dir = os.path.expanduser('~/InsureAI')\n",
    "model_dir = os.path.join(home_dir, \"models\", \"fine_tuned_model\") # Points to InsureAI/models/fine_tuned_model\n",
    "load_model = model_dir\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(load_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5b9cc-df1b-4edd-ab3f-63cc0a18c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce RAG\n",
    "\n",
    "class InsuranceRAGSystem:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        \"\"\"\n",
    "        Initialize RAG System with 4 key components:\n",
    "        1. Database connection - Access to live insurance data\n",
    "        2. Fine-tuned model - Your custom-trained insurance expert\n",
    "        3. Tokenizer - Processes text for the model\n",
    "        4. Query templates - Help translate questions to database queries\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.db_conn = sqlite3.connect(os.path.join(os.path.expanduser('~/InsureAI'), 'insurance.db'))\n",
    "        self.cursor = self.db_conn.cursor()\n",
    "        \n",
    "    def _understand_database(self):\n",
    "        \"\"\"\n",
    "        Discover database structure automatically:\n",
    "        - Identifies available tables\n",
    "        - Lists columns for each table\n",
    "        - Helps handle future table additions\n",
    "        \"\"\"\n",
    "        self.cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = self.cursor.fetchall()\n",
    "        schema = []\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            self.cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "            columns = [col[1] for col in self.cursor.fetchall()]\n",
    "            schema.append(f\"{table_name} ({', '.join(columns)})\")\n",
    "        return \"\\n\".join(schema)\n",
    "    \n",
    "    def _generate_sql(self, user_query):\n",
    "        \"\"\"\n",
    "        Convert natural language to SQL:\n",
    "        1. Uses your fine-tuned model's understanding\n",
    "        2. Considers current database structure\n",
    "        3. Creates safe, executable queries\n",
    "        \"\"\"\n",
    "        schema = self._understand_database()\n",
    "        prompt = f\"\"\"Convert this insurance question to SQL using the schema:\n",
    "        \n",
    "        Database Structure:\n",
    "        {schema}\n",
    "        \n",
    "        Question: {user_query}\n",
    "        SQL Query:\"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=200)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def _retrieve_data(self, sql_query):\n",
    "        \"\"\"\n",
    "        Safe database interaction:\n",
    "        1. Executes generated SQL\n",
    "        2. Returns results in natural language format\n",
    "        3. Handles errors gracefully\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.cursor.execute(sql_query)\n",
    "            columns = [desc[0] for desc in self.cursor.description]\n",
    "            results = self.cursor.fetchall()\n",
    "            return [dict(zip(columns, row)) for row in results[:3]]  # Return top 3 matches\n",
    "        except Exception as e:\n",
    "            print(f\"Database error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def ask(self, user_query):\n",
    "        \"\"\"\n",
    "        Complete RAG workflow:\n",
    "        1. Question → SQL\n",
    "        2. SQL → Database Results\n",
    "        3. Results + Question → Model Answer\n",
    "        \"\"\"\n",
    "        # Step 1: Generate SQL from question\n",
    "        sql_query = self._generate_sql(user_query)\n",
    "        print(f\"Generated SQL: {sql_query}\")\n",
    "        \n",
    "        # Step 2: Get relevant data\n",
    "        context_data = self._retrieve_data(sql_query)\n",
    "        if not context_data:\n",
    "            return \"I couldn't find relevant information for that question.\"\n",
    "        \n",
    "        # Step 3: Prepare augmented prompt\n",
    "        context_str = \"\\n\".join([str(item) for item in context_data])\n",
    "        full_prompt = f\"\"\"Use this insurance policy data to answer:\n",
    "        \n",
    "        {context_str}\n",
    "        \n",
    "        Question: {user_query}\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        # Step 4: Generate final response\n",
    "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=300)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Initialize RAG system with your fine-tuned model\n",
    "rag_system = InsuranceRAGSystem(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb14234a-c435-49c5-a823-769b61e8d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_response(prompt, max_length=100):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d640b6-14d7-492a-97e1-928280963854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Prompt:\n",
      "What is equal to 1+1?\n",
      "### Response:\n",
      "The sum of two ones is two, so 1+1 equals two.\n",
      "\n",
      "Alright, I'm trying to figure out how to respond to this instruction. The user provided a prompt asking \"What is equal to 1+1?\" and the response they wrote is: \"The sum of two ones is two, so 1\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is equal to 1+1?\"\n",
    "response = generate_response(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d2e612-a661-4f15-b551-56b491e9ec64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INSURANCE_QUERY] Information about Death B insurance ### Title: Death B insurance ### Abstract: Abstract ### Keywords: Keywords ### Date: Date\n",
      "\n",
      "I need to translate this into Chinese and put it into the format:\n",
      "信息 about Death B 保险\n",
      "摘要：摘要\n",
      "关键词：关键词\n",
      "日期：日期\n",
      "\n",
      "I have provided a sample in Chinese. Please help me translate the following information into Chinese, maintaining the same structure and formatting as the sample.\n",
      "信息 about Death B 保险\n",
      "摘要\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INSURANCE_QUERY] Information about Death B insurance ###\"\n",
    "response = generate_response(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8da94b-abfa-449f-bc24-751b7c01639a",
   "metadata": {},
   "source": [
    "Inference from original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19783fdb-f2cc-42ee-a72e-af6972643f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To query two table same column in SQL, you can use the SELECT statement with the column name as the common keyword. For example, to query two tables with the same column name 'id', you can use the following SQL query:\n",
      "\n",
      "SELECT id FROM table1 id, id FROM table2 id\n",
      "</think>\n",
      "\n",
      "To query two tables with the same column name in SQL, you can use the `SELECT` statement with the column name as a common keyword. Here's an example:\n",
      "\n",
      "```sql\n",
      "SELECT id FROM table1 id, id FROM table2 id\n",
      "```\n",
      "\n",
      "This query will return the `id` values from both `table1` and `table2`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)  # Move model to GPU\n",
    "max_new_tokens=1000\n",
    "\n",
    "# Custom stopping criteria for streaming\n",
    "class StopOnMaxTokens(StoppingCriteria):\n",
    "    def __init__(self, max_new_tokens):\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.current_tokens = 0\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        self.current_tokens += 1\n",
    "        return self.current_tokens >= self.max_new_tokens\n",
    "\n",
    "def generate_response_streaming(prompt, model, tokenizer, max_new_tokens=max_new_tokens):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Streaming-based generation\n",
    "    stop_criteria = StoppingCriteriaList([StopOnMaxTokens(max_new_tokens)])\n",
    "    \n",
    "    generated_ids = input_ids  # Start with prompt\n",
    "    # print(\"\\nAssistant:\", end=\" \", flush=True)\n",
    "\n",
    "    skip_tokens = {\"<think>\", \".</think>\", \".\", \"</think>\"}  # Define unwanted tokens\n",
    "    skip_mode = True  # Start in skip mode\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=generated_ids)  # Get logits\n",
    "            next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1, keepdim=True)  # Get top token\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:  # Stop on EOS token\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat((generated_ids, next_token_id), dim=1)  # Append token\n",
    "\n",
    "        # Decode new token properly\n",
    "        new_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "\n",
    "        if skip_mode:\n",
    "            if new_token.strip() in skip_tokens:  # Skip unwanted tokens\n",
    "                continue\n",
    "            else:\n",
    "                skip_mode = False  # Stop skipping once we see the real answer\n",
    "                new_token = new_token.lstrip() # Remove leading space from the first word\n",
    "\n",
    "        # Print token **without breaking spaces or newlines**\n",
    "        print(new_token, end=\"\", flush=True)\n",
    "\n",
    "        torch.cuda.synchronize()  # Ensure GPU processes before printing\n",
    "\n",
    "    # print(\"\\n\")  # Newline after response\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"How to query two table same column in SQL?\"\n",
    "generate_response_streaming(f'Answer directly without providing your thinking or reasoning. Keep your answer in {max_new_tokens - 100} max tokens.\\n\\n{prompt}', model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
